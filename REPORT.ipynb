{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Continuous Control - Report\n",
    "\n",
    "The project demonstrates how policy-based methods can be used to learn the optimal policy in a model-free Reinforcement Learning setting using a Unity environment, in which a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of the agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector is a number between -1 and 1. An agent choosing actions randomly can be seen in motion below:\n",
    "\n",
    "![random agent](assets/random_agent.gif) \n",
    "\n",
    "The following report is written in three parts:\n",
    "\n",
    "- **Implementation**\n",
    "\n",
    "- **Results**\n",
    "\n",
    "- **Ideas for improvement** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "The basic algorithm lying under the hood is an actor-critic method.\n",
    "\n",
    "\n",
    "###     Actor-Critic dual mechanism\n",
    "\n",
    "For each timestep _t,_ we do the following operations:\n",
    "\n",
    "Let __*S&nbsp;*__ be the current state. It is the  input for the  _Actor NN_.  The output is the action-value \n",
    "\n",
    "![](images/policy_pi.png)\n",
    "\n",
    "where ${\\pi}$ is the policy function,  i.e., the distribution of the actions. The _Critic NN_  gets the state __*S&nbsp;*__ as input and outputs      \n",
    "the state-value function __*v(S,w)*__ , that is the _expected total reward_ for the agent starting from state __*S&nbsp;*__. Here, _\\theta_ is    \n",
    "the vector parameter of the _Actor NN_, _w&nbsp;_ - the vector parameter of the _Critic NN_. The task is to train both networks, i.e.,   \n",
    "to find the optimal values for _\\theta_ and _w&nbsp;_.  By policy _\\pi_ we get the action _A&nbsp;_,  from the environment we get reward _R&nbsp;_   \n",
    "and the next state __*S'&nbsp;*__. Then we get _TD-estimate_: \n",
    " \n",
    "![](images/TD_estimate.png)\n",
    "\t\t \n",
    "Next, we use the _Critic_ to calculate the _advantage function_ _A(s, a)_:\n",
    "\n",
    "![](images/calc_advantage.png)\n",
    "\t\t\t\t \n",
    "Here, _\\gamma_ is the _discount factor_. The parameter _\\theta_ is updated by gradient ascent as follows:\n",
    "\n",
    "![](images/update_theta.png)\n",
    "\n",
    "The parameter _w&nbsp;_ is updated as follows:\n",
    "\n",
    "![](images/update_w.png)\n",
    "\t\t\n",
    "Here, ${\\alpha}$ (resp. ${\\beta}$) is the learning rate for the _Actor NN_ (resp. _Critic NN_).  Before we return to the next timestep we update the state _S&nbsp;_ and the operator _I&nbsp;_ by _discount factor_ \\gamma:\n",
    "\n",
    "![](images/next_state.png)\n",
    "\n",
    "At the start of the algorithm the operator _I_ should be initialized to the identity opeartor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  DDPG  Algorithm\n",
    "\n",
    "In this project we use _Algorithm DDPG_ (_Deep Deterministic Policy Gradient_).  _DDPG_ is an algorithm  which   \n",
    "concurrently learns a Q-function and a policy.  It uses off-policy data and the Bellman equation  to learn    \n",
    "the Q-function, and uses the Q-function to learn the policy. This dual mechanism is the _actor-critic method_. \n",
    "The DDPG algorithm uses two additional mechanisms: _Replay Buffer_ and _Soft Updates_. \n",
    "\n",
    "### Goal of DDPG Agent \n",
    "\n",
    "The environment for this project involves controlling a **double-jointed arm**, to reach target locations.     \n",
    "A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of      \n",
    "this agent is to maintain its position at the target location for as many time steps as possible. \n",
    "\n",
    "The observation space (i.e., state space) has 33 dimensions corresponding to position, rotation, velocity,    \n",
    "and angular velocities of the arm. The action space has 4 dimensions corresponding to torque applicable to    \n",
    "two joints. Every entry in the action vector should be a number between -1 and 1.\n",
    "\n",
    "\n",
    "The target network used for slow tracking of the learned network. We create a copy of the _actor_    and _critic_ networks:    \n",
    "_actor_\\__target_ (say, with the parameter vector _p'_) and _critic_\\__target_   (say, with the parameter vector _w'_). The weights of    \n",
    "these _target networks_ are updated by having   them the following track:    \n",
    "\n",
    "    p'  <--  p * \\tau + p' * (1 - \\tau)  \n",
    "    w'  <--  w * \\tau + w' * (1 - \\tau)\n",
    "\n",
    "We put the very small value for _\\tau_ (= 0.001). This means that the target values are constrained  to change slowly, greatly improving the stability of learning. This update is performed by function  _soft_\\__update_.   \n",
    "\n",
    "_\"This may slow learning, since the target network delays the propagation of value estimations.   \n",
    "However, in practice we found this was greatly outweighed by the stability of learning.\"     \n",
    "(\"Continuous control with deep reinforcement learning\", Lillicrap et al.,2015, arXiv:1509.02971)_  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPG Neural Networks\n",
    "\n",
    "The DDPG algorithm uses 4 neural networks: _actor_\\__target_, _actor_\\__local_, _critic_\\__target_ and _critic_\\__local_:\n",
    "\n",
    "    actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "    actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "\n",
    "    critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "    critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "\n",
    "classes _Actor_ and _Critic_ are provided by model.py. The typical behavior of _the actor_ and _the critic_\n",
    "is as follows:\n",
    "\n",
    "    actor_target(state) -> action\n",
    "    critic_target(state, action) -> Q-value\n",
    "    \n",
    "    actor_local(states) -> actions_pred\n",
    "    -critic_local(states, actions_pred) -> actor_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "There were many hyperparameters involved in the experiment. The value of each of them is given below:\n",
    "\n",
    "| Hyperparameter                      | Value |\n",
    "| ----------------------------------- | ----- |\n",
    "| Replay buffer size                  | 1e6   |\n",
    "| Batch size                          | 1024  |\n",
    "| $\\gamma$ (discount factor)          | 0.99  |\n",
    "| $\\tau$                              | 1e-3  |\n",
    "| Actor Learning rate                 | 1e-4  |\n",
    "| Critic Learning rate                | 3e-4  |\n",
    "| Update interval                     | 20    |\n",
    "| Update times per interval           | 10    |\n",
    "| Number of episodes                  | 500   |\n",
    "| Max number of timesteps per episode | 1000  |\n",
    "| Leak for LeakyReLU                  | 0.01  |\n",
    "\n",
    "\n",
    "Note that parameters LEARNING_PERIOD and UPDATE_FACTOR are critical for the **convergence** of the algorithm.    \n",
    "The corresponding code is in the function _step()_.    \n",
    "     \n",
    "     if len(self.memory) > BATCH_SIZE and timestep % LEARNING_PERIOD == 0:\n",
    "            for _ in range(UPDATE_FACTOR):\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "   \n",
    "\n",
    "  The trained agent can be seen in action below:\n",
    "\n",
    "  ![trained](assets/trained_agent.gif) \n",
    "\n",
    "\n",
    "\n",
    "  The best performance was achieved by **DDPG** where the reward of +30 was achieved in **337** episodes. I noticed how changing every single hyperparameter contributes significantly towards getting the right results and how hard it is to identify the ones which work. The plot of the rewards across episodes is shown below:\n",
    "\n",
    "  ![ddpg](assets/scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Possibly, the improve can be achieved by adding some layers to the neural networks Actor and Critic. Some papers state     \n",
    "   that Batch Normalization can accelerate Deep Network Training, \n",
    "   for example, [here](https://medium.com/@ilango100/batch-normalization-speed-up-neural-network-training-245e39a62f85) and [here](https://arxiv.org/pdf/1502.03167.pdf).\n",
    "\n",
    "2. Check different values for hyperparameters such as BATCH_SIZE, LR_ACTOR,  LR_CRITIC, LEARNING_PERIOD, UPDATE_FACTOR.    \n",
    " \n",
    "3. Instead of DDPG, other models can be considered, such as [PPO](https://openai.com/blog/openai-baselines-ppo/), \n",
    "   [A3C](https://blog.goodaudience.com/a3c-what-it-is-what-i-built-6b91fe5ec09c) and others.\n",
    "4. The Q-prop algorithm, which combines both off-policy  and on-policy learning, could be good one to try.\n",
    "\n",
    "5. General optimization techniques like cyclical learning rates and warm restarts could be useful as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
